{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbbcad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/efrem/dev/heatflask/backend/heatflask\n"
     ]
    }
   ],
   "source": [
    "# make sure we are working in module directory\n",
    "repo_root = !git rev-parse --show-toplevel\n",
    "module_path = repo_root[0] + \"/backend/heatflask\"\n",
    "%cd $module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab73779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Users.py\n",
    "\"\"\"\n",
    "***  For Jupyter notebook ***\n",
    "\n",
    "Paste one of these Jupyter magic directives to the top of a cell\n",
    " and run it, to do these things:\n",
    "\n",
    "  * %%cython --annotate\n",
    "      Compile and run the cell\n",
    "\n",
    "  * %load Users.py\n",
    "     Load Users.py file into this (empty) cell\n",
    "\n",
    "  * %%writefile Users.py\n",
    "      Write the contents of this cell to Users.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from logging import getLogger\n",
    "import datetime\n",
    "\n",
    "from DataAPIs import redis, init_collection\n",
    "import Index\n",
    "import Strava\n",
    "\n",
    "log = getLogger(__name__)\n",
    "log.propagate = True\n",
    "\n",
    "APP_NAME = \"heatflask\"\n",
    "COLLECTION_NAME = \"users\"\n",
    "CACHE_PREFIX = \"U:\"\n",
    "\n",
    "# Drop a user after a year of inactivity\n",
    "MONGO_TTL = 365 * 24 * 3600\n",
    "\n",
    "ADMIN = [15972102]\n",
    "\n",
    "DATA = {}\n",
    "\n",
    "\n",
    "async def get_collection():\n",
    "    if \"col\" not in DATA:\n",
    "        DATA[\"col\"] = await init_collection(\n",
    "            COLLECTION_NAME, force=False, ttl=MONGO_TTL, cache_prefix=CACHE_PREFIX\n",
    "        )\n",
    "    return DATA[\"col\"]\n",
    "\n",
    "\n",
    "def mongo_doc(\n",
    "    # From Strava Athlete record\n",
    "    id=None,\n",
    "    username=None,\n",
    "    firstname=None,\n",
    "    lastname=None,\n",
    "    profile_medium=None,\n",
    "    profile=None,-\n",
    "    measurement_preference=None,\n",
    "    city=None,\n",
    "    state=None,\n",
    "    country=None,\n",
    "    email=None,\n",
    "    # my additions\n",
    "    _id=None,\n",
    "    ts=None,\n",
    "    auth=None,\n",
    "    access_count=None,\n",
    "    private=None,\n",
    "):\n",
    "    doc = {\n",
    "        \"_id\": int(_id or id),\n",
    "        \"username\": username,\n",
    "        \"firstname\": firstname,\n",
    "        \"lastname\": lastname,\n",
    "        \"profile\": profile_medium or profile,\n",
    "        \"units\": measurement_preference,\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"country\": country,\n",
    "        \"email\": email,\n",
    "        #\n",
    "        \"ts\": ts or datetime.datetime.utcnow(),\n",
    "        \"access_count\": access_count or 0,\n",
    "        \"auth\": auth,\n",
    "        \"private\": private or False,\n",
    "    }\n",
    "\n",
    "    # Filter out any entries with None values\n",
    "    return {k: v for k, v in doc.items() if v is not None}\n",
    "\n",
    "\n",
    "async def add_or_update(**userdict):\n",
    "    users = get_collection()\n",
    "    doc = mongo_doc(**userdict)\n",
    "\n",
    "    # Creates a new user or updates an existing user (with the same id)\n",
    "    try:\n",
    "        return await users.update_one({\"_id\": doc[\"_id\"]}, doc, upsert=True)\n",
    "    except Exception:\n",
    "        log.exception(\"error adding/updating user: %s\", doc)\n",
    "\n",
    "async def access_token_update_callback(user_id, auth_info):\n",
    "    auth_update = {k: auth_info[k]  for k in (\"access_token\", \"refresh_token\", \"expires_at\")}\n",
    "    users = get_collection()\n",
    "    await users.update_one({\"_id\": user_id}, {\"$set\": {\"auth\": auth_update}}, upsert=True)\n",
    "        \n",
    "async def strava_client(user_id):\n",
    "    user_info = await get(user_id) \n",
    "    return Strava.AsyncClient(int(user_id), refresh_callback=access_token_update_callback, **user_info[\"auth\"])\n",
    "    \n",
    "    \n",
    "async def get(user_id):\n",
    "    users = get_collection()\n",
    "    try:\n",
    "        doc = await users.find_one({\"_id\": user_id})\n",
    "    except Exception:\n",
    "        log.exception(\"Failed mongodb query\")\n",
    "        doc = None\n",
    "    return doc\n",
    "\n",
    "\n",
    "async def delete(user_id):\n",
    "    users = get_collection()\n",
    "    uid = int(user_id)\n",
    "    try:\n",
    "        await users.delete_one({\"_id\": int(uid)})\n",
    "        await Index.delete_user_entries(uid)\n",
    "\n",
    "    except Exception:\n",
    "        log.exception(\"error deleting user %d\", user_id)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def dump(cls, attrs, **filter_by):\n",
    "        dump = [\n",
    "            {attr: getattr(user, attr) for attr in attrs}\n",
    "            for user in cls.query.filter_by(**filter_by)\n",
    "        ]\n",
    "        return dump\n",
    "\n",
    "    def index_count(self):\n",
    "        return Index.user_index_size(self)\n",
    "\n",
    "    def delete_index(self):\n",
    "        return Index.delete_user_entries(self)\n",
    "\n",
    "    def indexing(self, status=None):\n",
    "        # return or set the current state of index building\n",
    "        #  for this user\n",
    "        key = \"IDX:{}\".format(self.id)\n",
    "        if status is None:\n",
    "            return redis.get(key)\n",
    "\n",
    "        elif status is False:\n",
    "            return redis.delete(key)\n",
    "        else:\n",
    "            return redis.setex(key, 60, status)\n",
    "\n",
    "    def build_index(self, **args):\n",
    "        if OFFLINE:\n",
    "            return\n",
    "\n",
    "        return Index.import_user_index(user=self, out_query=args, yielding=False)\n",
    "\n",
    "    def query_activities(\n",
    "        self,\n",
    "        activity_ids=None,\n",
    "        exclude_ids=[],\n",
    "        limit=None,\n",
    "        after=None,\n",
    "        before=None,\n",
    "        streams=False,\n",
    "        owner_id=False,\n",
    "        update_index_ts=True,\n",
    "        cache_timeout=TTL_CACHE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # convert date strings to datetimes, if applicable\n",
    "        if before or after:\n",
    "            try:\n",
    "                if after:\n",
    "                    after = Utility.to_datetime(after)\n",
    "                if before:\n",
    "                    before = Utility.to_datetime(before)\n",
    "                if before and after:\n",
    "                    assert before > after\n",
    "            except AssertionError:\n",
    "                yield {\"error\": \"Invalid Dates\"}\n",
    "                return\n",
    "\n",
    "        client_query = Utility.cleandict(\n",
    "            dict(limit=limit, after=after, before=before, activity_ids=activity_ids)\n",
    "        )\n",
    "\n",
    "        self.strava_client = None\n",
    "        if not OFFLINE:\n",
    "            self.strava_client = StravaClient(user=self)\n",
    "            if not self.strava_client:\n",
    "                yield {\"error\": \"bad StravaClient. cannot import\"}\n",
    "\n",
    "        # exit if this query is empty\n",
    "        if not any([limit, activity_ids, before, after]):\n",
    "            log.debug(\"%s empty query\", self)\n",
    "            return\n",
    "\n",
    "        while self.indexing():\n",
    "            yield {\"idx\": self.indexing()}\n",
    "            gevent.sleep(0.5)\n",
    "\n",
    "        if self.index_count():\n",
    "            summaries_generator = Index.query(\n",
    "                user=self,\n",
    "                exclude_ids=exclude_ids,\n",
    "                update_ts=update_index_ts,\n",
    "                **client_query,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # There is no activity index and we are to build one\n",
    "            if OFFLINE:\n",
    "                yield {\"error\": \"cannot build index OFFLINE MODE\"}\n",
    "                return\n",
    "\n",
    "            if not self.strava_client:\n",
    "                yield {\"error\": \"could not create StravaClient. authenticate?\"}\n",
    "                return\n",
    "\n",
    "            summaries_generator = Index.import_user_index(\n",
    "                out_query=client_query, client=self.strava_client\n",
    "            )\n",
    "\n",
    "            if not summaries_generator:\n",
    "                log.info(\"Could not build index for %s\", self)\n",
    "                return\n",
    "\n",
    "        # Here we introduce a mapper that readys an activity summary\n",
    "        #  (with or without streams) to be yielded to the client\n",
    "        now = datetime.utcnow()\n",
    "        timer = Timer()\n",
    "        self.abort_signal = False\n",
    "\n",
    "        def export(A):\n",
    "            if self.abort_signal:\n",
    "                return\n",
    "            if not A:\n",
    "                return A\n",
    "\n",
    "            # get an actvity object ready to send to client\n",
    "            if \"_id\" not in A:\n",
    "                # This is not an activity. It is\n",
    "                #  an error message or something\n",
    "                #  so pass it on.\n",
    "                return A\n",
    "\n",
    "            ttl = (A[\"ts\"] - now).total_seconds() + TTL_INDEX\n",
    "            A[\"ttl\"] = max(0, int(ttl))\n",
    "\n",
    "            try:\n",
    "                ts_local = A.pop(\"ts_local\")\n",
    "                ts_UTC = A.pop(\"ts_UTC\")\n",
    "\n",
    "                ts_local = int(Utility.to_datetime(ts_local).timestamp())\n",
    "                ts_UTC = int(Utility.to_datetime(ts_UTC).timestamp())\n",
    "            except Exception:\n",
    "                log.exception(\"%s, %s\", ts_local, ts_UTC)\n",
    "\n",
    "            # A[\"ts\"] received by the client will be a tuple (UTC, diff)\n",
    "            #  where UTC is the time of activity (GMT), and diff is\n",
    "            #  hours offset so that\n",
    "            #   ts_local= UTC + 3600 * diff\n",
    "            A[\"ts\"] = (ts_UTC, (ts_local - ts_UTC) / 3600)\n",
    "\n",
    "            if owner_id:\n",
    "                A.update(dict(owner=self.id, profile=self.profile))\n",
    "\n",
    "            return A\n",
    "\n",
    "        # if we are only sending summaries to client,\n",
    "        #  get them ready to export and yield them\n",
    "        count = 0\n",
    "        if not streams:\n",
    "            for A in map(export, summaries_generator):\n",
    "                if A and \"_id\" in A:\n",
    "                    count += 1\n",
    "                abort_signal = yield A\n",
    "                if abort_signal:\n",
    "                    summaries_generator.send(abort_signal)\n",
    "                    break\n",
    "            log.debug(\"%s exported %s summaries in %s\", self, count, timer.elapsed())\n",
    "            return\n",
    "\n",
    "        #  summaries_generator yields activity summaries without streams\n",
    "        #  We want to attach a stream to each one, get it ready to export,\n",
    "        #  and yield it.\n",
    "\n",
    "        to_export = gevent.queue.Queue(maxsize=512)\n",
    "        if self.strava_client:\n",
    "            to_import = gevent.queue.Queue(maxsize=512)\n",
    "            batch_queue = gevent.queue.Queue()\n",
    "        else:\n",
    "            to_import = FakeQueue()\n",
    "\n",
    "        def import_activity_streams(A):\n",
    "            if not (A and \"_id\" in A):\n",
    "                return A\n",
    "\n",
    "            if self.abort_signal:\n",
    "                log.debug(\"%s import %s aborted\", self, A[\"_id\"])\n",
    "                return\n",
    "\n",
    "            start = time.time()\n",
    "            _id = A[\"_id\"]\n",
    "            log.debug(\"%s request import %s\", self, _id)\n",
    "\n",
    "            A = Activities.import_streams(\n",
    "                self.strava_client, A, batch_queue=batch_queue\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            log.debug(\"%s response %s in %s\", self, _id, round(elapsed, 2))\n",
    "\n",
    "            if A:\n",
    "                import_stats[\"n\"] += 1\n",
    "                import_stats[\"dt\"] += elapsed\n",
    "\n",
    "            elif A is False:\n",
    "                import_stats[\"err\"] += 1\n",
    "                if import_stats[\"err\"] >= MAX_IMPORT_ERRORS:\n",
    "                    log.info(\"%s Too many import errors. quitting\", self)\n",
    "                    self.abort_signal = True\n",
    "                    return\n",
    "            else:\n",
    "                import_stats[\"emp\"] += 1\n",
    "\n",
    "            return A\n",
    "\n",
    "        # this is where the action happens\n",
    "        stats = dict(n=0)\n",
    "        import_stats = dict(n=0, err=0, emp=0, dt=0)\n",
    "\n",
    "        import_pool = gevent.pool.Pool(IMPORT_CONCURRENCY)\n",
    "        aux_pool = gevent.pool.Pool(3)\n",
    "\n",
    "        if self.strava_client:\n",
    "            # this is a lazy iterator that pulls activites from import queue\n",
    "            #  and generates activities with streams. Roughly equivalent to\n",
    "            #   imported = ( import_activity_streams(A) for A in to_import )\n",
    "            imported = import_pool.imap_unordered(import_activity_streams, to_import)\n",
    "\n",
    "            def handle_imported(imported):\n",
    "                for A in imported:\n",
    "                    if A and not self.abort_signal:\n",
    "                        to_export.put(A)\n",
    "\n",
    "            def imported_done(result):\n",
    "                if import_stats[\"n\"]:\n",
    "                    import_stats[\"resp\"] = round(\n",
    "                        import_stats[\"dt\"] / import_stats[\"n\"], 2\n",
    "                    )\n",
    "                log.debug(\"%s done importing\", self)\n",
    "                to_export.put(StopIteration)\n",
    "\n",
    "            # this background job fills export queue\n",
    "            # with activities from imported\n",
    "            aux_pool.spawn(handle_imported, imported).link(imported_done)\n",
    "\n",
    "        # background job filling import and export queues\n",
    "        #  it will pause when either queue is full\n",
    "        chunks = Utility.chunks(summaries_generator, size=BATCH_CHUNK_SIZE)\n",
    "\n",
    "        def process_chunks(chunks):\n",
    "            for chunk in chunks:\n",
    "                handle_raw(chunk)\n",
    "\n",
    "        def handle_raw(raw_summaries):\n",
    "            for A in Activities.append_streams_from_db(raw_summaries):\n",
    "                handle_fetched(A)\n",
    "\n",
    "        def handle_fetched(A):\n",
    "            if not A or self.abort_signal:\n",
    "                return\n",
    "            if \"time\" in A:\n",
    "                to_export.put(A)\n",
    "                stats[\"n\"] += 1\n",
    "            elif \"_id\" not in A:\n",
    "                to_export.put(A)\n",
    "            else:\n",
    "                to_import.put(A)\n",
    "\n",
    "        def raw_done(dummy):\n",
    "            # The value of result will be False\n",
    "            log.debug(\"%s done with raw summaries. elapsed=%s\", self, timer.elapsed())\n",
    "\n",
    "            if self.strava_client:\n",
    "                to_import.put(StopIteration)\n",
    "            else:\n",
    "                to_export.put(StopIteration)\n",
    "\n",
    "        aux_pool.spawn(process_chunks, chunks).link(raw_done)\n",
    "\n",
    "        count = 0\n",
    "        for A in map(export, to_export):\n",
    "            self.abort_signal = yield A\n",
    "            count += 1\n",
    "\n",
    "            if self.abort_signal:\n",
    "                log.info(\"%s received abort_signal. quitting...\", self)\n",
    "                summaries_generator.send(abort_signal)\n",
    "                break\n",
    "\n",
    "        elapsed = timer.elapsed()\n",
    "        stats[\"dt\"] = round(elapsed, 2)\n",
    "        stats = Utility.cleandict(stats)\n",
    "        import_stats = Utility.cleandict(import_stats)\n",
    "        if import_stats:\n",
    "            batch_queue.put(StopIteration)\n",
    "            write_result = Activities.set_many(batch_queue)\n",
    "            try:\n",
    "                import_stats[\"t_rel\"] = round(import_stats.pop(\"dt\") / elapsed, 2)\n",
    "                import_stats[\"rate\"] = round(import_stats[\"n\"] / elapsed, 2)\n",
    "            except Exception:\n",
    "                pass\n",
    "            log.info(\"%s import %s\", self, import_stats)\n",
    "\n",
    "        if \"n\" in stats:\n",
    "            log.info(\"%s fetch %s\", self, stats)\n",
    "\n",
    "        if import_stats:\n",
    "            stats[\"import\"] = import_stats\n",
    "\n",
    "        if (\"n\" in stats) or import_stats:\n",
    "            EventLogger.new_event(msg=\"{} fetch {}\".format(self, stats))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95907cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"f\": 3, \"g\": 4}\n",
    "for a in d:\n",
    "    print(f\"{a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3973769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
