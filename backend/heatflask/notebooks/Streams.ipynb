{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we are working in module directory\n",
    "repo_root = !git rev-parse --show-toplevel\n",
    "module_path = repo_root[0] + \"/backend/heatflask\"\n",
    "%cd $module_path\n",
    "\n",
    "import sys\n",
    "__package__ = \"heatflask\"\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "    \n",
    "# Make cells wider\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load StreamCodecs.py\n",
    "# Here we define a custom encoding/compression scheme for streams\n",
    "# Run-Length-Diff encoding\n",
    "#\n",
    "#  It is RLE on successive differences, which in our case are small enough to\n",
    "#  be 8 bit integers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Nums = list[int] | list[float] | np.ndarray\n",
    "RLDEncoded = bytes\n",
    "\n",
    "\n",
    "def positive_non_decreasing(vals: Nums) -> bool:\n",
    "    lastv = vals[0]\n",
    "    if lastv < 0:\n",
    "        return False\n",
    "\n",
    "    i = 1\n",
    "    while i < len(vals):\n",
    "        if vals[i] < lastv:\n",
    "            return False\n",
    "        lastv = vals[i]\n",
    "        i += 1\n",
    "    return True\n",
    "\n",
    "\n",
    "def rld_encode(vals: Nums) -> RLDEncoded:\n",
    "    vals = (\n",
    "        np.fromiter((v + 0.5 for v in vals), dtype=\"i4\", count=len(vals))\n",
    "        if type(vals[0]) is float\n",
    "        else np.array(vals, dtype=\"i4\")\n",
    "    )\n",
    "\n",
    "    increasing = positive_non_decreasing(vals)\n",
    "    my_dtype = np.uint8 if increasing else np.int8\n",
    "    rl_marker = 255 if increasing else -128\n",
    "    max_reps = 254 if increasing else 126\n",
    "\n",
    "    n = len(vals)\n",
    "    encoded = np.empty(n, dtype=my_dtype)\n",
    "    reps = 0\n",
    "    j = 0\n",
    "\n",
    "    v = vals[1]\n",
    "    d = v - vals[0]\n",
    "\n",
    "    for i in range(2, len(vals)):\n",
    "        next_v = vals[i]\n",
    "        next_d = next_v - v\n",
    "\n",
    "        if (d == next_d) and (reps < max_reps):\n",
    "            reps += 1\n",
    "\n",
    "        else:\n",
    "            if reps == 0:\n",
    "                encoded[j] = d\n",
    "                j += 1\n",
    "            elif reps <= 2:\n",
    "                reps += 1\n",
    "                while reps:\n",
    "                    encoded[j] = d\n",
    "                    j += 1\n",
    "                    reps -= 1\n",
    "            else:\n",
    "                encoded[j] = rl_marker\n",
    "                encoded[j + 1] = d\n",
    "                encoded[j + 2] = reps + 1\n",
    "                j += 3\n",
    "                reps = 0\n",
    "        d = next_d\n",
    "        v = next_v\n",
    "\n",
    "    if reps == 0:\n",
    "        encoded[j] = d\n",
    "        j += 1\n",
    "    elif reps == 1:\n",
    "        encoded[j] = d\n",
    "        encoded[j + 1] = d\n",
    "        j += 2\n",
    "    else:\n",
    "        encoded[j] = rl_marker\n",
    "        encoded[j + 1] = d\n",
    "        encoded[j + 2] = reps + 1\n",
    "        j += 3\n",
    "\n",
    "    ntype = b\"\\x01\" if increasing else b\"\\x00\"\n",
    "    firstval = np.array(vals[0], dtype=np.int16).tobytes()\n",
    "    bytesdata = ntype + firstval + encoded[:j].tobytes()\n",
    "    return bytesdata\n",
    "\n",
    "\n",
    "def decoded_length(enc: np.ndarray, rl_marker: int) -> int:\n",
    "    L = 1\n",
    "    i = 0\n",
    "    while i < len(enc):\n",
    "        if enc[i] == rl_marker:\n",
    "            L += enc[i + 2]\n",
    "            i += 3\n",
    "        else:\n",
    "            L += 1\n",
    "            i += 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def rld_decode(enc: RLDEncoded, dtype=np.int32) -> Nums:\n",
    "    ntype = np.frombuffer(enc, dtype=\"i1\", count=1, offset=0)[0]\n",
    "    start_val = np.frombuffer(enc, dtype=\"i2\", count=1, offset=1)[0]\n",
    "    enc_diffs = np.frombuffer(enc, dtype=\"i1\" if ntype == 0 else \"u1\", offset=3)\n",
    "\n",
    "    increasing = ntype != 0\n",
    "\n",
    "    rl_marker = 255 if increasing else -128\n",
    "    L = decoded_length(enc_diffs, rl_marker)\n",
    "\n",
    "    decoded = np.empty(L, dtype=dtype)\n",
    "    decoded[0] = start_val\n",
    "    cumsum = start_val\n",
    "    i = 0  # enc_diffs counter\n",
    "    j = 1  # decoded counter\n",
    "    while i < len(enc_diffs):\n",
    "        if enc_diffs[i] == rl_marker:\n",
    "            d = enc_diffs[i + 1]\n",
    "            reps = enc_diffs[i + 2]\n",
    "            endreps = j + reps\n",
    "            while j < endreps:\n",
    "                cumsum += d\n",
    "                decoded[j] = cumsum\n",
    "                j += 1\n",
    "            i += 3\n",
    "        else:\n",
    "            cumsum += enc_diffs[i]\n",
    "            decoded[j] = cumsum\n",
    "            i += 1\n",
    "            j += 1\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f48dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Streams.py\n",
    "\"\"\"\n",
    "Functions and constants pertaining to the Streams data store.  Each activity\n",
    "has the streams time, latlng, and altitude.\n",
    "\n",
    "***  For Jupyter notebook ***\n",
    "Paste one of these Jupyter magic directives to the top of a cell\n",
    " and run it, to do these things:\n",
    "    %%cython --annotate      # Compile and run the cell\n",
    "    %load Streams.py         # Load Streams.py file into this (empty) cell\n",
    "    %%writefile Streams.py   # Write the contents of this cell to Streams.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from logging import getLogger\n",
    "import msgpack\n",
    "import polyline\n",
    "import asyncio\n",
    "import types\n",
    "from typing import TypedDict, Awaitable, AsyncGenerator, Coroutine, cast\n",
    "\n",
    "from . import DataAPIs\n",
    "from .DataAPIs import db\n",
    "from . import Strava\n",
    "from . import StreamCodecs\n",
    "from .Users import UserField as U\n",
    "\n",
    "\n",
    "log = getLogger(__name__)\n",
    "log.setLevel(\"DEBUG\")\n",
    "log.propagate = True\n",
    "\n",
    "COLLECTION_NAME = \"streams_v0\"\n",
    "CACHE_PREFIX = \"S:\"\n",
    "\n",
    "SECS_IN_HOUR = 60 * 60\n",
    "SECS_IN_DAY = 24 * SECS_IN_HOUR\n",
    "\n",
    "MONGO_TTL = int(os.environ.get(\"MONGO_STREAMS_TTL\", 10)) * SECS_IN_DAY\n",
    "REDIS_TTL = int(os.environ.get(\"REDIS_STREAMS_TTL\", 4)) * SECS_IN_HOUR\n",
    "OFFLINE = os.environ.get(\"OFFLINE\")\n",
    "\n",
    "myBox = types.SimpleNamespace(collection=None)\n",
    "\n",
    "\n",
    "async def get_collection():\n",
    "    if myBox.collection is None:\n",
    "        myBox.collection = await DataAPIs.init_collection(\n",
    "            COLLECTION_NAME, ttl=MONGO_TTL, cache_prefix=CACHE_PREFIX\n",
    "        )\n",
    "    return myBox.collection\n",
    "\n",
    "\n",
    "POLYLINE_PRECISION = 6\n",
    "\n",
    "\n",
    "class EncodedStreams(TypedDict):\n",
    "    t: StreamCodecs.RLDEncoded\n",
    "    a: StreamCodecs.RLDEncoded\n",
    "    p: str\n",
    "\n",
    "\n",
    "PackedStreams = bytes\n",
    "\n",
    "\n",
    "def encode_streams(rjson: Strava.Streams) -> PackedStreams:\n",
    "    \"\"\"compress stream data\"\"\"\n",
    "    enc: EncodedStreams = {\n",
    "        \"t\": StreamCodecs.rld_encode(rjson[\"time\"][\"data\"]),\n",
    "        \"a\": StreamCodecs.rld_encode(rjson[\"altitude\"][\"data\"]),\n",
    "        \"p\": polyline.encode(rjson[\"latlng\"][\"data\"], POLYLINE_PRECISION),\n",
    "    }\n",
    "    return msgpack.packb(enc)\n",
    "\n",
    "\n",
    "def decode_streams(msgpacked_streams: PackedStreams):\n",
    "    \"\"\"de-compress stream data\"\"\"\n",
    "    d: EncodedStreams = msgpack.unpackb(msgpacked_streams)\n",
    "    return {\n",
    "        \"time\": StreamCodecs.rld_decode(d[\"t\"], dtype=\"u2\"),\n",
    "        \"altitude\": StreamCodecs.rld_decode(d[\"a\"], dtype=\"i2\"),\n",
    "        \"latlng\": polyline.decode(d[\"p\"], POLYLINE_PRECISION),\n",
    "    }\n",
    "\n",
    "\n",
    "class StreamsDoc(TypedDict):\n",
    "    _id: int\n",
    "    mpk: PackedStreams\n",
    "    ts: datetime.datetime\n",
    "\n",
    "\n",
    "def mongo_doc(activity_id: int, packed: PackedStreams, ts=None) -> StreamsDoc:\n",
    "    return {\n",
    "        \"_id\": int(activity_id),\n",
    "        \"mpk\": packed,\n",
    "        \"ts\": ts or datetime.datetime.now(),\n",
    "    }\n",
    "\n",
    "\n",
    "def cache_key(aid: int):\n",
    "    return f\"{CACHE_PREFIX}{aid}\"\n",
    "\n",
    "\n",
    "StreamsQueryResult = tuple[int, PackedStreams]\n",
    "\n",
    "\n",
    "async def strava_import(\n",
    "    activity_ids: list[int], **user\n",
    ") -> AsyncGenerator[StreamsQueryResult, bool]:\n",
    "    uid = int(user[U.ID])\n",
    "\n",
    "    strava = Strava.AsyncClient(uid, **user[U.AUTH])\n",
    "    await strava.update_access_token()\n",
    "    coll = await get_collection()\n",
    "\n",
    "    mongo_docs = []\n",
    "    now = datetime.datetime.now()\n",
    "    aiterator = strava.get_many_streams(activity_ids)\n",
    "\n",
    "    async with db.redis.pipeline(transaction=True) as pipe:\n",
    "        async for aid, streams in aiterator:\n",
    "            packed = encode_streams(streams)\n",
    "\n",
    "            # queue packed streams to be redis cached\n",
    "            pipe = pipe.setex(cache_key(aid), REDIS_TTL, packed)\n",
    "\n",
    "            mongo_docs.append(mongo_doc(aid, packed, ts=now))\n",
    "\n",
    "            abort_signal = yield aid, packed\n",
    "\n",
    "            if abort_signal:\n",
    "                await Strava.AsyncClient.abort(aiterator)\n",
    "                break\n",
    "\n",
    "        await pipe.execute()\n",
    "    await coll.insert_many(mongo_docs)\n",
    "\n",
    "\n",
    "async def aiter_query(\n",
    "    activity_ids: list[int], user=None\n",
    ") -> AsyncGenerator[StreamsQueryResult, bool]:\n",
    "    if not activity_ids:\n",
    "        return\n",
    "    #\n",
    "    # First we check Redis cache\n",
    "    #\n",
    "    t0 = time.perf_counter()\n",
    "    keys = [cache_key(aid) for aid in activity_ids]\n",
    "    redis_response = await db.redis.mget(keys)\n",
    "\n",
    "    # Reset TTL for those cached streams that were hit\n",
    "    async with db.redis.pipeline(transaction=True) as pipe:\n",
    "        for k, val in zip(keys, redis_response):\n",
    "            if val:\n",
    "                pipe = pipe.expire(k, REDIS_TTL)\n",
    "        await pipe.execute()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    local_result: list[StreamsQueryResult] = [\n",
    "        (a, s) for a, s in zip(activity_ids, redis_response) if s\n",
    "    ]\n",
    "    log.debug(\n",
    "        \"retrieved %d streams from Redis in %d\", len(local_result), (t1 - t0) * 1000\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Next we query MongoDB for streams that were not in Redis\n",
    "    #\n",
    "    # activity IDs of cache misses\n",
    "    activity_ids = [a for a, s in zip(activity_ids, redis_response) if not s]\n",
    "    if activity_ids:\n",
    "        # Next we query MongoDB for any cache misses\n",
    "        t0 = time.perf_counter()\n",
    "        streams = await get_collection()\n",
    "        query = {\"_id\": {\"$in\": activity_ids}}\n",
    "        exclusions = {\"ts\": False}\n",
    "\n",
    "        cursor = streams.find(query, projection=exclusions)\n",
    "        mongo_result = [(doc[\"_id\"], doc[\"mpk\"]) async for doc in cursor]\n",
    "        local_result.extend(mongo_result)\n",
    "        mongo_result_ids = [_id for _id, mpk in mongo_result]\n",
    "\n",
    "        # Cache the mongo hits\n",
    "        async with db.redis.pipeline(transaction=True) as pipe:\n",
    "            for aid, s in mongo_result:\n",
    "                pipe = pipe.setex(cache_key(aid), REDIS_TTL, s)\n",
    "            await pipe.execute()\n",
    "\n",
    "        # Update TTL for mongo hits\n",
    "        await streams.update_many(\n",
    "            {\"_id\": {\"$in\": mongo_result_ids}},\n",
    "            {\"$set\": {\"ts\": datetime.datetime.utcnow()}},\n",
    "        )\n",
    "        elapsed = (time.perf_counter() - t0) * 1000\n",
    "        log.debug(\"retrieved %d streams from Mongo in %d\", len(mongo_result), elapsed)\n",
    "\n",
    "        activity_ids = list(set(activity_ids) - set(mongo_result_ids))\n",
    "\n",
    "    streams_import = None\n",
    "    first_fetch = None\n",
    "    if activity_ids and (user is not None) and (not OFFLINE):\n",
    "        # Start a fetch process going. We will get back to this...\n",
    "        t0 = time.perf_counter()\n",
    "        streams_import = strava_import(activity_ids, **user)\n",
    "        first_fetch = asyncio.create_task(cast(Coroutine, streams_import.__anext__()))\n",
    "\n",
    "    # Yield all the results from Redis and Mongo\n",
    "    for item in local_result:\n",
    "        abort_signal = yield item\n",
    "        if abort_signal:\n",
    "            log.info(\"Local Streams query aborted\")\n",
    "            if streams_import:\n",
    "                await Strava.AsyncClient.abort(streams_import)\n",
    "            break\n",
    "\n",
    "    if streams_import:\n",
    "        # Now we yield results of fetches as they come in\n",
    "        item1: StreamsQueryResult = await cast(Awaitable, first_fetch)\n",
    "        abort_signal = yield item1\n",
    "        imported_items = [item1]\n",
    "\n",
    "        if not abort_signal:\n",
    "            async for item in streams_import:\n",
    "                imported_items.append(item)\n",
    "                abort_signal = yield item\n",
    "                if abort_signal:\n",
    "                    break\n",
    "\n",
    "        if abort_signal:\n",
    "            Strava.AsyncClient.abort(streams_import)\n",
    "            log.info(\"Remote Streams query aborted\")\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        log.debug(\n",
    "            \"retrieved %d streams from Strava in %d\",\n",
    "            len(imported_items),\n",
    "            (t1 - t0) * 1000,\n",
    "        )\n",
    "        imported_ids = set(aid for aid, mpk in imported_items)\n",
    "        missing_ids = set(activity_ids) - imported_ids\n",
    "        if missing_ids:\n",
    "            log.info(\"unable to import streams for %s\", missing_ids)\n",
    "\n",
    "\n",
    "async def query(**kwargs) -> list[StreamsQueryResult]:\n",
    "    return [s async for s in aiter_query(**kwargs)]\n",
    "\n",
    "\n",
    "async def delete(activity_ids: list[int]):\n",
    "    if not activity_ids:\n",
    "        return\n",
    "    streams = await get_collection()\n",
    "    await streams.delete_many({\"_id\": {\"$in\": activity_ids}})\n",
    "    keys = [cache_key(aid) for aid in activity_ids]\n",
    "    await db.redis.delete(*keys)\n",
    "\n",
    "\n",
    "async def clear_cache():\n",
    "    streams_keys = await db.redis.keys(cache_key(\"*\"))\n",
    "    if streams_keys:\n",
    "        return await db.redis.delete(*streams_keys)\n",
    "\n",
    "\n",
    "def stats():\n",
    "    return DataAPIs.stats(COLLECTION_NAME)\n",
    "\n",
    "\n",
    "def drop():\n",
    "    return DataAPIs.drop(COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f09182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "await DataAPIs.connect(None, None)\n",
    "\n",
    "N_FETCH = 15\n",
    "\n",
    "from . import Index\n",
    "result = await Index.query(limit=N_FETCH)\n",
    "activity_ids = [d[\"_id\"] for d in result[\"docs\"]]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99733c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from . import Users\n",
    "from . import Strava\n",
    "\n",
    "admin = await Users.get(Users.ADMIN[0])\n",
    "admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = await query(activity_ids=activity_ids, user=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = zip(*q)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59126d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "await stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "await DataAPIs.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
